{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorge-martinez-gil/colab-notebooks/blob/main/GraphCodeBERT%2BFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-code"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Runtime/Resource benchmarking for:\n",
        "- Baseline: GraphCodeBERT classifier\n",
        "- Ours: GraphCodeBERT + additional scalar feature\n",
        "\n",
        "No wandb. Works on old/new transformers.\n",
        "Dataset JSON fields: code1, code2, score (0/1), output (float)\n",
        "\n",
        "Author: Jorge Martinez-Gil\n",
        "\"\"\"\n",
        "\n",
        "# ---- Disable external loggers ----\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"\n",
        "\n",
        "import time, json, random, urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "MODEL_NAME = \"microsoft/graphcodebert-base\"\n",
        "DATASET_URL = \"https://www.jorgemar.com/data/data2.json\"\n",
        "DATASET_PATH = \"data2.json\"\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "EVAL_STEPS = 500\n",
        "SAVE_STEPS = 500\n",
        "WARMUP_STEPS = 500\n",
        "WEIGHT_DECAY = 0.01\n",
        "SEED = 42\n",
        "LAT_SAMPLES = 64\n",
        "\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    urllib.request.urlretrieve(DATASET_URL, DATASET_PATH)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=-1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": p, \"recall\": r}\n",
        "\n",
        "class CodePairDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_length, use_feature=True):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.use_feature = use_feature\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        enc = self.tokenizer(item[\"code1\"], item[\"code2\"], truncation=True,\n",
        "                             padding=\"max_length\", max_length=self.max_length,\n",
        "                             return_tensors=\"pt\")\n",
        "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        enc[\"labels\"] = torch.tensor(item[\"score\"], dtype=torch.long)\n",
        "        enc[\"output_feature\"] = torch.tensor(float(item[\"output\"]) if self.use_feature else 0.0)\n",
        "        return enc\n",
        "\n",
        "class GCBaseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.cls = nn.Linear(self.enc.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, output_feature=None):\n",
        "        h = self.enc(input_ids, attention_mask).last_hidden_state[:,0]\n",
        "        logits = self.cls(h)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels) if labels is not None else None\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "def main():\n",
        "    set_seed(SEED)\n",
        "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    ds = CodePairDataset(DATASET_PATH, tok, MAX_LENGTH)\n",
        "    model = GCBaseline()\n",
        "    args = TrainingArguments(output_dir=\"./out\", per_device_train_batch_size=BATCH_SIZE,\n",
        "                             num_train_epochs=EPOCHS, report_to=[\"none\"])\n",
        "    Trainer(model=model, args=args, train_dataset=ds).train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "GraphCodeBERT+Features.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
